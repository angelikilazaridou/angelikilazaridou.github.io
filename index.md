---
layout: default
---



<h2 id="about-me">About me</h2>
<p><img src="../resourses/me.jpg" width="250" height="200" style="float:right">Iâ€™m Angeliki, a graduate student, working with <a href="http://clic.cimec.unitn.it/marco">Marco Baroni</a> on multimodal semantics,  at the <a href="http://clic.cimec.unitn.it">CLIC Lab</a>  of the Center for Mind/Brain Sciences of the University of Trento, Italy
(<a href="https://www.google.com/maps/place/Roveret://www.google.com/maps/place/38068+Rovereto+TN,+It%C3%A1lie/@47.2603133,11.7074777,5z/data=!4m2!3m1!1s0x47820ec143127041:0x6a9664123aebfadf">map</a>).<br />
Before that, I did an MSc in <a href="http://www.coli.uni-saarland.de">Computational Linguistics at the University of Saarland</a> working with <a href="http://ivan-titov.org">Ivan Titov</a> and <a href="http://www.uni-trier.de/index.php?id=46381">Caroline Sporleder</a> on Sentiment Analysis.<br />
And before that, I was a Computer Science BSc student working with <a href="http://users.iit.demokritos.gr/~paliourg">Georgios Paliouras</a> on Sentiment Analysis.</p>


News
---------
* March 2015: Our work *From visual attributes to adjectives through decompositional distributional semantics* with Georgiana, Adam and Marco got accepted at TACL
* February 2015: Our work *Combining Language and Vision with a Multimodal Skip-gram Model*, with Nghia and Marco, got selected for an oral presentation at NAACL 
* February 2015: I was selected as a Facebook Fellowship Finalist 

My research
------------
My primary research interests are in the area of natural language
processing (NLP), and specifically, in multimodal semantics. 
This area of computational semantics aims at making purely text-based models of meaning interact with other modalities, such as visual and sensorimotor, and thus simulate how humans acquire and process information and draw inferences using all available modalities. 
Specifically, I focus on interfacing the linguistic modality with its visual counterpart through the cross-modal learning paradigm.

So far, I've work on demonstrating how  cross-modal mapping can be used in novel computational tasks in computer vision (**attribute
learning**) and NLP (**learning word embeddings**). 
Given the promising results of cross-modal mapping in several tasks, I've looked into ways to improve the learning of mapping algorithms by exploiting ideas from other discriminative tasks as well as from domain adaptation.

*When the thunder rolled, the wampimuk ran behind the oak*. Most of us, after hearing this sentence would create visual expectations about how *wampimuks* look like. Humans are able to reason across modalities even when exposed to limited linguistic input, which is obviously the case for novel concepts and rare words. Currently, I'm investigating to which extend distributional semantics and cross-modal mapping can come together to simulate **cross-modal reasoning** in the way that humans do.

In the near future, I'm planning to demonstrate that cross-modal mapping combined with recent advances in computer vision can provide a window to computational models of meanings through **concept image generation**.


