---
layout: default
---

<div style="text-align:center"><img src="../resourses/me.jpg" alt="Image of me" align="center"/></div>

About me
--------
I'm Angeliki, a graduate student, working with [Marco Baroni](http://clic.cimec.unitn.it/marco),  at the [CLIC Lab](http://clic.cimec.unitn.it)  of the Center for Mind/Brain Sciences of the University of Trento, Italy 
([map](https://www.google.com/maps/place/Roveret://www.google.com/maps/place/38068+Rovereto+TN,+It%C3%A1lie/@47.2603133,11.7074777,5z/data=!4m2!3m1!1s0x47820ec143127041:0x6a9664123aebfadf)).\\
Before that, I did an MSc in [Computational Linguistics at the University of Saarland](http://www.coli.uni-saarland.de) working on Sentiment Analysis with [Ivan Titov](http://ivan-titov.org) and [Caroline Sporleder](http://www.uni-trier.de/index.php?id=46381).\\
And before that, I was a Computer Science BSc student at the University of Athens working with [George Paliouras](http://users.iit.demokritos.gr/~paliourg) on Sentiment Analysis. 

News
---------
* February 2015: Our work *Combining Language and Vision with Multimodal Skip-gram Model*, with Nghia The Pham and Marco Baroni, got selected for an oral presentation at NAACL 
* February 2015: I was selected as a Facebook Fellowship Finalist 

My research
------------
My primary research interests are in the area of natural language
processing (NLP), and specifically, in multimodal semantics. 
This area of computational semantics aims at making purely text-based models of meaning interact with other modalities, such as visual and sensorimotor, and thus simulate how humans acquire and process information and draw inferences using all available modalities. 
Specifically, I focus on interfacing the linguistic modality with its visual counterpart through the cross-modal learning paradigm.

So far, I've work on demonstrating how  cross-modal mapping can be used in novel computational tasks in computer vision (e.g., attribute
learning) and NLP (e.g., learning word embeddings). 
Given the promising results of cross-modal mapping in several tasks, I've looked into ways to improve the learning of mapping algorithms by exploiting ideas from other discriminative tasks as well as from domain adaptation.

Currently, I'm trying to validate through behavioral multimodal experiments that cross-modal mapping is a cognitively plausible inference mechanism. 

In the near future, I'm planning to demonstrate that cross-modal mapping combined with recent advances in computer vision can provide a window to computational models of meanings through concept-image generation.


